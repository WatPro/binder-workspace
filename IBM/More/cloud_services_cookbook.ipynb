{"nbformat_minor": 0, "cells": [{"source": "# Cloud Services Cookbook\n\nThis notebook will review how to connect to and make use of cloud service providers.\n\n* Amazon S3\n* Dropbox\n* Twitter", "cell_type": "markdown", "metadata": {}}, {"source": "# Amazon Web Services (AWS)\n\nAWS is Amazon's cloud framework. [boto](https://aws.amazon.com/sdk-for-python/) is the AWS SDK for Python.  boto provides Python APIs for many AWS services including Amazon S3, Amazon EC2, Amazon DynamoDB, and more. It supports Python 2 and 3.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_help>", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Setup\nInstall the [boto](https://aws.amazon.com/sdk-for-python/) Python SDK library, which wraps the Amazon REST APIs.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_setup>\n!pip install boto", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_setup>\n!pip install filechunkio", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Amazon S3\nAmazon Simple Storage Service (S3) provides file storage in the cloud.  Files are encapsulated as objects.  Amazon S3 enables developers to store, retrieve and delete objects. It also allows setting permissions and metadata on files through the object interfaces.\n\nTo be able to perform operations on Amazon S3, you must first have an AWS account with S3 enabled.  You must also provide your AWS credentials in order to connect.  You can obtain your credentials by following these steps: \n\n1. Login to [AWS](https://portal.aws.amazon.com/billing/signup).\n2. Click `Account`.\n3. Click the `Security Credentials` link.\n4. Extract the key ID and secret access key. (You will use them in the code cells below to access and connect to Amazon S3).\n5. Grant Access to read/write on buckets and its contents.  \n    * Go to [AWS->IAM Management Console](https://console.aws.amazon.com/iam/)\n    * Click `Dashboard` > `Users` and select the user. \n    * Click on `User Policies` and add `PowerUserAccess` and `AdministratorAccess`.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_s3_help>", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Create Bucket\nCreate a [bucket](http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html), the container used in Amazon S3 for data storage, and put an object in that bucket.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_s3_create_bucket>\nimport boto\nimport time\n\naws_access_key_id = \"ACCESS KEY ID\"\naws_secret_access_key = \"SECRET ACCESS KEY\"\ns3 = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\n\n# Create a new bucket. Buckets must have a globally unique name (not just\n# unique to the account, so choose a good name).\nbucket = s3.create_bucket(\"globally_unique_name_for_s3_bucket\")\n# Create a new key/value pair.\nkey = bucket.new_key('mykey')\nkey.set_contents_from_string(\"Hello World!\")\n# Sleep to ensure the data is eventually there.\ntime.sleep(2)\n# Retrieve the contents of the key.\nprint key.get_contents_as_string()\n# Delete key.\nkey.delete()\n# Delete bucket.\nbucket.delete()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Store Data with Keys  \nAny type of data file can be stored in Amazon S3.  Each file is stored as a key/value pair, where the key is unique within a bucket.  \n\nThe `Key` object is used in `boto` to keep track of data stored in Amazon S3.  Store new data in Amazon S3 by creating a new `Key` object and adding a value:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_s3_get_key>\nimport boto\nfrom boto.s3.key import Key\nimport time \n\naws_access_key_id = \"ACCESS KEY ID\"\naws_secret_access_key = \"SECRET ACCESS KEY\"\ns3 = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\nb = s3.create_bucket('globally_unique_name_for_s3_bucket')\nk = b.new_key('newS3key')\nk.set_contents_from_string('This is a test of S3.')\n# Sleep to ensure the data is eventually there.\ntime.sleep(2)\n# Retrieve contents of key.\nprint k.get_contents_as_string()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Upload/Download file to/from Amazon S3\nWith a key, you can upload and download files to and from Amazon S3.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_s3_key_contents_from_filename>\nimport boto\nfrom boto.s3.key import Key\n\naws_access_key_id = \"ACCESS KEY ID\"\naws_secret_access_key = \"SECRET ACCESS KEY\"\ns3 = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\nb = s3.create_bucket('globally_unique_name_for_s3_bucket')\nk = Key(b)\nk.key = 'my test file'\n# create test file on disk\ntestfile_name = \"uploaded_aws_test_file.txt\"\nwith open(\"/resources/data/\" + testfile_name, 'w') as f:\n    f.write(\"Test file content\")\n# upload test file to S3\nk.set_contents_from_filename(testfile_name)\nprint('uploaded sucessfully')\n# download test file from S3 with new file name\nk.get_contents_to_filename('downloaded_aws_test_file.txt')\nprint('downloaded sucessfully')", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Keys are validated to see if they exist. This can be disabled with `validate=False`", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_s3_validate_key_false>\nimport boto\n\naws_access_key_id = \"ACCESS KEY ID\"\naws_secret_access_key = \"SECRET ACCESS KEY\"\ns3 = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\nb = s3.get_bucket('globally_unique_name_for_s3_bucket') # substitute your bucket name here\n# Will hit the API to check if it exists.\npossible_key = b.get_key('newS3key') # substitute your key name here\nprint(possible_key)\nprint('\\n')\n# Won't hit the API.\nkey_we_know_is_there = b.get_key('newS3key', validate=False)\nprint(key_we_know_is_there)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Upload Large Data  \nAmazon S3 supports splitting large files into smaller chunks.  All chunks are uploaded and then Amazon S3 combines them into a single file.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_s3_upload_filechunkio>\nimport math, os\nimport boto\nfrom filechunkio import FileChunkIO\n\naws_access_key_id = \"ACCESS KEY ID\"\naws_secret_access_key = \"SECRET ACCESS KEY\"\ns3 = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\nb = s3.get_bucket('globally_unique_name_for_s3_bucket')\n# Get file info\ntestfile_name = \"uploaded_aws_test_file.txt\"\nwith open(\"/resources/data/\" + testfile_name, 'w') as f:\n    f.write(\"Test file content\")\n# source_path = 'uploaded_aws_test_file.txt'\nfile_size = os.stat(testfile_name).st_size\n# Create a multipart upload request\nmp = b.initiate_multipart_upload(os.path.basename(testfile_name))\n# Use a chunk size of 50 MiB (feel free to change this)\nchunk_size = 52428800\nchunk_count = int(math.ceil(file_size / chunk_size))\n# Send the file parts, using FileChunkIO to create a file-like object\n# that points to a certain byte range within the original file. \n# S3 set bytes to never exceed the original file size.\nfor i in range(chunk_count + 1):\n    offset = chunk_size * i\n    bytes = min(chunk_size, file_size - offset)\n    with FileChunkIO(testfile_name, 'r', offset=offset,\n                     bytes=bytes) as fp:\n        mp.upload_part_from_file(fp, part_num=i + 1)\n# Finish the upload\nmp.complete_upload()\nprint('Upload Complete')", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Get Bucket\nGet a bucket.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_s3_get_bucket>\nimport boto\n\naws_access_key_id = \"ACCESS KEY ID\"\naws_secret_access_key = \"SECRET ACCESS KEY\"\ns3 = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\nmybucket = s3.get_bucket('globally_unique_name_for_s3_bucket') # Substitute in your bucket name, might need an entire path \nmybucket.list()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Delete Bucket  \nDelete a bucket.  \n\n**Note**: you must first delete the contents of the bucket, or an error is raised.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_s3_delete_bucket>\nimport boto\n\naws_access_key_id = \"ACCESS KEY ID\"\naws_secret_access_key = \"SECRET ACCESS KEY\"\ns3 = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\nfull_bucket = s3.get_bucket('globally_unique_name_for_s3_bucket')\n# It's full of keys. Delete them all.\nfor key in full_bucket.list():\n    key.delete()\n# The bucket is empty now. Delete it.\ns3.delete_bucket('globally_unique_name_for_s3_bucket')\nprint('Deleted')", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### List All Buckets  \nRetrieve all created buckets. This returns a `ResultSet` object. The `ResultSet` can be used as a `sequence` or `list` type object to retrieve `Bucket` objects.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_s3_list_bucket>\nimport boto\n\naws_access_key_id = \"ACCESS KEY ID\"\naws_secret_access_key = \"SECRET ACCESS KEY\"\ns3 = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\n#get\nrs = s3.get_all_buckets()\n#list them out\nlen(rs)\nfor b in rs:\n    print b.name\n#<listing of available buckets>\n#show an individual one\nb = rs[0]", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Access Control   \nThe Access Control List (ACL) allows setting read/write permissions on objects. There are two ways to set the `ACL` for an object:\n\n1. Create a custom ACL that grants specific rights to specific users. At the moment, the users that are specified within grants have to be registered users of Amazon Web Services so this isn\u2019t as useful or as general as it could be.\n2. Use a \u201ccanned\u201d access control policy. There are four canned policies defined:\n    * `private`: Owner gets `FULL_CONTROL`. No one else has any access rights.\n    * `public-read`: Owners gets `FULL_CONTROL` and the anonymous principal is granted `READ` access.\n    * `public-read-write`: Owner gets `FULL_CONTROL` and the anonymous principal is granted `READ` and `WRITE` access.\n    * `authenticated-read`: Owner gets `FULL_CONTROL` and any principal authenticated as a registered Amazon S3 user is granted `READ` access.\n\nSet the bucket to `public-read` and the ACL for a key object the bucket as an argument.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:amazon_s3_get_acl>\nimport boto\n\naws_access_key_id = \"ACCESS KEY ID\"\naws_secret_access_key = \"SECRET ACCESS KEY\"\ns3 = boto.connect_s3(aws_access_key_id, aws_secret_access_key)\n# the used bucket must be previously created\nb = s3.get_bucket('globally_unique_name_for_s3_bucket')\n# the used key must be previously created\nk = b.get_key('newS3key')\n# set acl \nb.set_acl('public-read', 'newS3key')\n# set acl bucket with key\nk.set_acl('public-read')\n# get acl \nacp = b.get_acl()\n# display perms\nfor grant in acp.acl.grants:\n    print grant.permission, grant.display_name, grant.email_address, grant.id", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "#Dropbox\n\nDropbox offers two main APIs:\n\nThe [Core API](https://www.dropbox.com/developers/core) allows read and write access on Dropbox, such as downloading /uploading files, and modifying/deleting items. \n\nThe [DataStore API](https://www.dropbox.com/developers/datastore) is focused more on applications that connect to Dropbox to sync data such as calendars, structured data like lists etc.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:dropbox_help>", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "##Setup\n\nInstall the [dropbox](https://pypi.python.org/pypi/dropbox) Python client library, which wraps the Dropbox REST APIs.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:dropbox_setup>\n!pip install dropbox", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## Authentication\n\nDropbox uses Oauth to authenticate users.  To access Dropbox from a notebook, you must first register the notebook as an application to be able to authenticate.\n\n1. Go to the [App Console](https://www.dropbox.com/developers/apps). \n2. Click on \"Create App\"\n3. Choose \"Dropbox API App\", then choose the following options: \n    * What type of data does your app need to store on Dropbox?: **Files and Datastores**\n    * Can your app be limited to its own folder?: **No**\n    * What type of files does your app need access to?: **All file types**\n4. Select a name for your app (must be unique). \n5. Click \"Create App\".\n6. Note the app key and app secret. You will need these to authorize access.  (Don't share these with anyone). \n\nThe Dropbox client has a class called [DropboxOAuth2FlowNoRedirect](https://www.dropbox.com/developers/core/docs/python#DropboxOAuth2FlowNoRedirect) that our notebook app can use instead of providing an HTTP callback url during the OAuth handshake.  This class presents the end user with an authorization url, from which the user can obtain an authorization code.  From the [Dropbox Documentation](https://www.dropbox.com/developers/core/start/python):\n>\"This URL can ask the user to authorize your app. The URL will be printed and will ask the user to press the `Enter` key to confirm that they've authorized the app. However, in real-world apps, it's recommended to automatically send the user to the authorization URL and pass in a callback URL so that the user is seamlessly redirected back to the app after pressing a button.\"\n\nAfter providing the authorization code to the Dropbox client, the notebook can use the client to access the user's account and perform actions.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_auth>\nimport dropbox\n\n# Get your app key and secret from the Dropbox developer website\napp_key = 'app_key'\napp_secret = 'app_secret'\n# oauth dance\nflow = dropbox.client.DropboxOAuth2FlowNoRedirect(app_key, app_secret)\nauthorize_url = flow.start()\nprint '1. Go to: ' + authorize_url\nprint '2. Click \"Allow\" (you may have to log in first)'\nprint '3. Copy the authorization code.'\n# click on the link, opens new window, get the code, \n# paste the code in the cell as plaintext, has a timer so be quick!\ncode = raw_input(\"Enter the authorization code here: \").strip()\n# This will fail if the user enters an invalid authorization code\naccess_token, user_id = flow.finish(code)\n#test access to core api\nclient = dropbox.client.DropboxClient(access_token)\nprint 'linked account: ', client.account_info()", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "##Dropbox Core API\n\nThe Core API allows not just uploading and downloading files but other options such as search, revisions, and restoring files. \nThe APi has a functionality to get a printed dictionary with the results of its commands. If the command fails, the API will return an error with the details.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_core_help>", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Upload File  \n\n`put_file()` takes a path pointing to where the file should be uploaded to Dropbox, and then a file-like object or string to be uploaded there.  \n\n**Create on your local machine a draft.txt.**  \nThis command will upload it as magnum-opus.txt (change to what name you wish).\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_core_put_file>\nimport dropbox\n\ntestfile_name = \"draft.txt\"\nwith open(\"/resources/data/\" + testfile_name, 'w') as f:\n    f.write(\"Test file content\")\nf = open('draft.txt', 'rb')\nresponse = client.put_file('/magnum-opus.txt', f)\n#print response with details\nprint \"Uploaded:\", response", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Create Folder  \n\n`file_create_folder(path)`  \nSelect the path of the folder to be created. \n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_core_create_folder>\nimport dropbox\n\nresponse = client.file_create_folder('/mypythonapp')\nprint \"uploaded:\", response", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Copy or Move File  \n\nCopy a file with `file_copy('from_path', 'to_path')`\n\nTo move, use `file_move('from_path', 'to_path')`\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_core_file_copy>\nimport dropbox\n\nresponse = client.file_copy('/magnum-opus.txt','/Dropbox/mypythonapp/')\nprint \"copied:\", response", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Folder metadata  \n\nGet the info of an entire folder by using the `metadata()` call\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_core_metadata>\nimport dropbox\n\nfolder_metadata = client.metadata('/')\nprint \"metadata:\", folder_metadata", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Download File  \n\nThe `get_file_and_metadata` method downloads the file and its metadata.  \nThe method also returns the metadata at its current revision. Every time a change is made to the file, the `rev` field of the file's metadata changes as well.  \nBy saving the revision when the file is downloaded, it will be easy to tell if it's been modified by another device and whether to choose to download the newer revision.\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_core_get_file_and_metadata>\nimport dropbox\n\nf, metadata = client.get_file_and_metadata('/magnum-opus.txt')\nout = open('magnum-opus.txt', 'wb')\nout.write(f.read())\nout.close()\nprint metadata", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Search files  \n\n`search(path, query, file_limit=1000, include_deleted=False)`  \n\nIn which  \n* `path`: folder to search  \n* `query`: minimum 3 characters, what to look for, case sensitive.  \n* `file_limit`: up to 1000 files to display  \n* `include_deleted`: whether to include deleted files in the search results  \n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_core_search>\nimport dropbox\n\nresponse = client.search('/','magnum', file_limit=100, include_deleted= False)\nprint \"Results:\", response", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Share File/Folder  \n\n`share(path, short_url=True)`\n\nCreate a shareable link to a file or folder.\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_core_share>\nimport dropbox\n\nresponse = client.share('/magnum-opus.txt', short_url=True)\nprint \"Results:\", response", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Upload Large Files\n\nUpload large files with the `ChunkedUploader` object. It will break up the file into chunks and upload it in bits. \n\n`get_chunked_uploader(file_obj, length)`  \n* `file_obj` is the file to be uploaded  \n* `length`, number of bytes to upload  \n\nUse a `try/catch` block to upload; the SDK leaves the error handling and retry logic to the developer to implement.\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> \n\n**Create a text file called big.txt**", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:dropbox_core_chunked_uploader>\nimport dropbox\n\nbigFile = open(\"big.txt\", 'rb')\nsize= 1024\n\nuploader = client.get_chunked_uploader(bigFile, size)\nprint \"uploading: \", size\nwhile uploader.offset < size:\n    try:\n        upload = uploader.upload_chunked()\n        print('finished1')\n    except rest.ErrorResponse, e:\n            # implement error handling and retry logic here\n        print('Error trying to upload')\nuploader.finish('/bigFile.txt')\nprint('Uploaded')", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Delete file  \n\nDelete a file or folder with `file_delete('path')`  \n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_core_file_delete>\nimport dropbox\n#delete\nresponse = client.file_delete('/magnum-opus.txt')\nprint \"Results:\", response", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "##Datastore API\nThe Datastore API supports multiple platforms, offline access, and automatic conflict resolution.\n\n**Client and datastore manager**  \n\nThe client lets the app start the authentication process to link with a user's Dropbox account. Once it's linked to an account (`dropbox_setup` code), the client can create a datastore manager, which can open datastores, list datastores, etc.\n\n**Datastores and tables**  \n\nDatastores are containers for an app's data. Each datastore contains a set of tables, and each table is a collection of records. A table allows to query existing records or insert new ones.\n\nAny datastore with a shareable ID can be shared by assigning roles to principals, creating an access control list. Any Dropbox account with the correct permissions will then be able to open the shared datastore by ID.\n\n**Records**  \n\nRecords are how an app stores data. Each record consists of a set of fields, each with a name and a value. Values can be simple objects, like strings, integers, and booleans, or they can be lists of simple objects. A record has an ID and can have any number of fields.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:dropbox_datastore_help>", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Create datastore and table \n\nWith an access token in hand, the next step is to open the default datastore. Each app has its own default datastore per user. \n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_datastore_get_table>\nimport dropbox\nfrom dropbox.datastore import DatastoreError, DatastoreManager, Date, Bytes\n\n#define a datastore\nmanager = DatastoreManager(client)\ndatastore = manager.open_default_datastore()\nprint(datastore)\nprint('\\n')\n#create a table named 'tasks'\ntasks_table = datastore.get_table('tasks')\nprint(tasks_table)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Upload Record  \n\nA record is a set of name and value pairs called fields. Records in the same table can have different combinations of fields; there's no schema on the table which contains them.\n\nThe Python SDK provides a method called `transaction` that uploads data to the server keeping track of conflicts.\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_datastore_record_transaction>\nimport dropbox\nfrom dropbox.datastore import DatastoreError, DatastoreManager, Date, Bytes\n\n#define a datastore\nmanager = DatastoreManager(client)\ndatastore = manager.open_default_datastore()\nprint(datastore)\nprint('\\n')\n#create a table named 'tasks'\ntasks_table = datastore.get_table('My tasks')\nprint(tasks_table)\n#load record and commit to dropbox with transaction\ndef do_insert():\n    tasks_table.insert(taskname='Buy milk', completed=False)\ndatastore.transaction(do_insert, max_tries=4)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Access, Edit and Delete a record  \n\nReview uploaded datastores in the [Developer Website](https://www.dropbox.com/developers/apps/datastores)  \n\n`.get()`, `.set()`, `.delete()`   \nAccess, Edit and Delete a DataStore\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_datastore_open_get_delete>\nimport dropbox\nfrom dropbox.datastore import DatastoreError, DatastoreManager, Date, Bytes\n\n#define a datastore\nmanager = DatastoreManager(client)\ndatastore = manager.open_default_datastore()\nprint(datastore)\nprint('\\n')\n#access a datastore\ntask_name = first_task.get('taskname')\n#edit a datastore\ndef do_update():\n    first_task.set('completed', True)\ndatastore.transaction(do_update, max_tries=4)\n#delete a datastore\ndef do_delete():\n    first_task.delete()\ndatastore.transaction(do_delete, max_tries=4)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Query Record  \n\nThe query method takes a set of conditions that the fields of a record must match to be returned in the result set. All records must have a field with that name and that field's value must be exactly equal to the specified value, case sensitive too.\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_datastore_table_query>\nimport dropbox\nfrom dropbox.datastore import DatastoreError, DatastoreManager, Date, Bytes\n\n#define a datastore\nmanager = DatastoreManager(client)\ndatastore = manager.open_default_datastore()\nprint(datastore)\nprint('\\n')\ntasks = tasks_table.query(completed=False)\nfor task in tasks:\n    print task.get('taskname')", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Share datastore  \n\nThe Datastore API allows sharing a datastore across multiple Dropbox accounts.\n\nAny user who has the datastore ID and the appropriate permissions may then open the datastore.  \nView the access control list at any time for a datastore as a mapping of roles applied to principals using the `list_roles()` method. Find out the current user's role with the `get_effective_role()` method.\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run the `dropbox_auth` step first to authorize with Dropbox. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: dropbox_datastore_share_set_role>\nimport dropbox\nfrom dropbox.datastore import DatastoreError, DatastoreManager, Date, Bytes\n\n# Shareable datastore\ndatastore = manager.create_datastore()\n#set a role, more info in the API docs\ndatastore.set_role(Datastore.PUBLIC, Datastore.EDITOR)\nprint(datastore)\nprint('\\n \\n')\n#paste data store id below!\ndatastore_id = \ndatastore1 = manager.open_datastore(datastore_id)\nprint(datastore1)\nprint('\\n \\n')", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "**Review the Datastore Python API docs [here](https://www.dropbox.com/developers/datastore/docs/python)**.", "cell_type": "markdown", "metadata": {}}, {"source": "# Twitter\nTwitter offers [REST APIs](https://dev.twitter.com/rest/public) and [Streaming APIs](https://dev.twitter.com/streaming/overview) to connect and download tweets.  The documentation can be found [here](https://dev.twitter.com/overview/documentation).\n\nThe Python community has created wrappers around the Twitter API:  \n\n* [python-twitter](https://github.com/bear/python-twitter) (Apache 2.0 License)  \n* [tweepy](https://github.com/tweepy/tweepy)  (MIT License)  \n\nWe will use **tweepy** in this notebook, which wraps the [Twitter REST API](https://dev.twitter.com/rest/public).  ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help:twitter_help>", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Setup\nInstall the [tweepy](https://github.com/tweepy/tweepy) Python client library, which wraps the Twitter REST APIs.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: twitter_setup>\n!pip install tweepy", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### Authorize\nThe Twitter API uses OAuth for authentication. To allow an application to access Twitter, you must register the app on Twitter's web site.  You also need to obtain app keys and app secrets to authenticate.   \n\n1. Go to https://apps.twitter.com/ (Sign in or create an account) \n1. Create New App. Provide the necessary information.\n1. Select the application and click on \"API Keys and access tokens\" tab\n1. Copy and paste the following into the code cell below:\n    * access token/secret: Used to make API requests on your account's behalf.\n    * consumer key, consumer secret: Allows the app to read and write user data.\n    \nFirst, we need to authorize to Twitter.  We only need to do this once per session.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: twitter_auth>\nimport tweepy\nfrom tweepy import OAuthHandler\n\n#replace with your data!\nconsumer_key = 'key'\nconsumer_secret = 'secret'\naccess_token = 'token'\naccess_token_secret = 'token_secret'\n#oauth dance\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n# Authorize via Twitter authorization url\n# And since we aren't using a web callback url, \n# We have to enter the auth code manually\nredirect_url = auth.get_authorization_url()\nprint(redirect_url)\nverifier = raw_input(\"Enter the authorization code here: \").strip()\n# Get access token and save it\ntry:\n    auth.get_access_token(verifier)\nexcept tweepy.TweepError:\n    print 'Error! Failed to get access token.'\nauth.set_access_token(access_token, access_token_secret)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Get Tweets\nUse Twitter API retrieve tweets from our own (home) timeline.\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run `twitter_auth` step first to authorize with Twitter. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: twitter_tweets>\n\n# Construct the API instance\napi = tweepy.API(auth)\n# Get tweets\npublic_tweets = api.home_timeline()\nfor tweet in public_tweets:\n    print tweet.text", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Search Tweets  \n\nSearch the text of tweets.\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run `twitter_auth` step first to authorize with Twitter. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: twitter_search>\nimport tweepy\n\n# Construct the API instance\napi = tweepy.API(auth)\n#search and iterate\npublic_tweets = api.search('the interview')\nfor tweet in public_tweets:\n    print tweet.text", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###List Followers \n\nPaginate through a user's followers.\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run `twitter_auth` step first to authorize with Twitter. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: twitter_lookup_users>\nimport tweepy\nimport itertools\n\n# Construct the API instance\napi = tweepy.API(auth)\n#replace 'user' string with a valid Twitter handle\nfollowers = api.followers_ids('user')\n\ndef paginate(iterable, page_size):\n    while True:\n        i1, i2 = itertools.tee(iterable)\n        iterable, page = (itertools.islice(i1, page_size, None),\n                list(itertools.islice(i2, page_size)))\n        if len(page) == 0:\n            break\n        yield page\nfor page in paginate(followers, 100):\n    results = api.lookup_users(user_ids=page)\n    for result in results:\n        print result.screen_name", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Trending Topics\n\nReturns the top **ten** topics that are currently trending on Twitter. The response includes the time of the request, the name of each trend, and the url to the Twitter Search results page for that topic. This recipe will strip and only print the hashtags with the # symbol.\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run `twitter_auth` step first to authorize with Twitter. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: twitter_trends_place>\nimport tweepy\n\n# Construct the API instance\napi = tweepy.API(auth)\n# trending topics are limited to geographical location\n# use the geo code from https://developer.yahoo.com/geo/geoplanet/\ntrending = api.trends_place(1)\n# trending is a list with only one element in it, which is a \n# dict which we'll put in data.\ndata = trending[0] \n# grab the trends\ntrends = data['trends']\n# grab the name from each trend; place in a list\nnames = [trend['name'] for trend in trends]\n# put all the names together with a ', ' separating them\ntrendsName = ', '.join(names)\nprint(trendsName)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Cursor  \n\nTweepy has the `Cursor` object to iterate through timelines, user lists, direct messages, etc.\nThe `cursor` object returns a deserialized `json` object. \n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run `twitter_auth` step first to authorize with Twitter. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: twitter_cursor>\nimport tweepy\n\napi = tweepy.API(auth)\ntweepy.Cursor(api.user_timeline, id=\"twitter\")\nfor page in tweepy.Cursor(api.user_timeline).pages():\n    # page is a list of tweets, with replies\n    print(page)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###List All\nGet all lists that a user is subscribed to.\n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run `twitter_auth` step first to authorize with Twitter. </div> ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: twitter_list_all>\nimport tweepy\n\napi = tweepy.API(auth)\nfor status in api.lists_all(id = 'user_auth'):\n    print pprint(status)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "###Stream API\n\nThe streaming APIs allow to pull real-time data from Twitter. \n\nTwitter offers several streaming parameters, each customized to certain use cases.\n\n* _Public streams_-\tStreams of the public data in Twitter. Example: hashtags\n* _User streams_-\tSingle-user streams. Example: tweets from a specific user.\n* _Site streams_-\tThe multi-user version of user streams. \n\nThe streaming process gets the Tweets and performs any organization needed before storing the result into a data store. \n\n<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**Note:** You must run `twitter_auth` step first to authorize with Twitter. </div> \n\n<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">**Warning** This will run indefinitely until you stop the cell. </div>", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#<help: twitter_stream>\nimport tweepy\nfrom tweepy.streaming import StreamListener\nfrom tweepy import Stream\nimport json\n\n\n# This is the listener, resposible for receiving data\nclass StdOutListener(tweepy.StreamListener):\n    def on_data(self, data):\n        # Twitter returns data in JSON format - we need to decode it first\n        decoded = json.loads(data)\n        # Also, we convert UTF-8 to ASCII ignoring all bad characters sent by users\n        print '@%s: %s' % (decoded['user']['screen_name'], decoded['text'].encode('ascii', 'ignore'))\n        print ''\n        return True\n    def on_error(self, status):\n        print status\n        \nprint \"Showing all new tweets for #python:\"\nstream = tweepy.Stream(auth, StdOutListener())\nstream.filter(track=['python'])", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}