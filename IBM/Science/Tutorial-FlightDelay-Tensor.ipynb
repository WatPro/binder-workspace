{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>Predicting Flight Delays</h1>\n",
    "<h2 align=center>with Apache Spark and TensorFlow</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will use the popular [Flights Dataset](http://stat-computing.org/dataexpo/2009/the-data.html) to analyze and predict flight delays in airports based on past flight records. We will show how you can use __Jupyter Notebooks, TensorFlow and Apache Spark__ to read, explore, analyze and visualize your results.  \n",
    "\n",
    "This tutorial is intended for readers who wants to use TensorFlow in Jupyter Notebooks. We use a __Jupyter Notebook__ to write an interactive Python code, use __Spark__ to distribute the preprocessing phase, and use __TensorFlow__ to efficiently make the model.\n",
    "\n",
    "__TensorFlow__ is an open-source machine learning library for numerical computation using data flow graphs. If you can express your computation as a data flow graph, you can use TensorFlow. TensorFlow library is already installed in your Data Scientist Workbench so you can simply import it into your notebook.\n",
    "\n",
    "Although __Spark ML library and TensorFlow__ both have been designed based on the DataFlow paradigm of parallel computation, the distributed version of TensorFlow has not been released yet. This means that it can be run only on one node as of now. That is, with Spark an RDD is distributed on many nodes of cluster, whereas TensorFlow sits on one node. Therefore, after preprocessing the flight dataset using Spark, we will collect and convert the final results into a numpy matrix (on one node) and use that for modeling with TensorFlow.\n",
    "\n",
    "For this dataset, we will only look at the flights in 2007 - this is still 7 million flights! \n",
    "\n",
    "In this notebook, we will build **a classification model to predict airline delay from historical flight data.**  We are going to train a model to look at flights and predict whether they have delay or not.  \n",
    "\n",
    "First, we need to import some Python packages that we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data\n",
    "To import data into your Data Scientist Workbench (DSWB), you can take either one of these actions:\n",
    "\n",
    "1) Paste the following link into the sidebar of your DSWB:\n",
    "https://share.datascientistworkbench.com/#/api/v1/workbench/10.115.89.160/shares/QBNwgXam7veFKl7/airline2007.csv\n",
    "\n",
    "OR\n",
    "\n",
    "2) Run the following cell to download it directly to you DSWB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Will download airline2007.csv if file not yet downloaded\n",
    "\n",
    "if os.path.isfile(\"/resources/airline2007.csv\") != True:\n",
    "    #If file does not already exist, download it, unzip, then delete zipped file\n",
    "    !wget --quiet --output-document  /resources/airline2007.csv.bz2 http://stat-computing.org/dataexpo/2009/2007.csv.bz2\n",
    "    !bzip2 -d /resources/airline2007.csv.bz2\n",
    "    !rm /resources/airline2007.csv.bz2\n",
    "    print \"Downloaded to /resources/airline2007.csv\"\n",
    "else:\n",
    "    #If file already exists\n",
    "    print \"airline2007.csv already exists under /resources/airline2007.csv\"\n",
    "    print \"You can continue to the next cell.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile('/resources/airline2007.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data\n",
    "In this section, we remove the header of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFileRDD = textFile.map(lambda x: x.split(','))\n",
    "header = textFileRDD.first()\n",
    "\n",
    "textRDD = textFileRDD.filter(lambda r: r != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataframe from RDD\n",
    "A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in Python, but with richer optimizations under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(r):\n",
    "    try:\n",
    "        x=Row(Year=int(r[0]),\\\n",
    "          Month=int(r[1]),\\\n",
    "          DayofMonth=int(r[2]),\\\n",
    "          DayOfWeek=int(r[3]),\\\n",
    "          DepTime=int(float(r[4])), \\\n",
    "          CRSDepTime=int(r[5]),\\\n",
    "          ArrTime=int(float(r[6])),\\\n",
    "          CRSArrTime=int(r[7]), \\\n",
    "          UniqueCarrier=r[8],\\\n",
    "          DepDelay=int(float(r[15])),\\\n",
    "          Origin=r[16],\\\n",
    "          Dest=r[17], \\\n",
    "          Distance=int(float(r[18])))  \n",
    "    except:\n",
    "        x=None  \n",
    "    return x\n",
    "\n",
    "rowRDD = textRDD.map(lambda r: parse(r)).filter(lambda r:r != None)\n",
    "airline_df = sqlContext.createDataFrame(rowRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we add a new column to our data frame, **DepDelayed**, a binary variable:\n",
    "- **True**, for flights that have > 15 minutes of delay\n",
    "- **False**, for flights that have <= 15 minutes of delay\n",
    "\n",
    "We will later use **Depdelayed** as the target/label column in the classification process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline_df = airline_df.withColumn('DepDelayed', airline_df['DepDelay']>15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add a new column, __Hour__, to determine the hour of flight (0 to 24)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define hour function to obtain hour of day\n",
    "def hour_ex(x): \n",
    "    h = int(str(int(x)).zfill(4)[:2])\n",
    "    return h\n",
    "\n",
    "# register as a UDF \n",
    "f = udf(hour_ex, IntegerType())\n",
    "\n",
    "#CRSDepTime: scheduled departure time (local, hhmm)\n",
    "airline_df = airline_df.withColumn('hour', f(airline_df.CRSDepTime))\n",
    "airline_df.registerTempTable(\"airlineDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "Let's do some exploration of this dataset.  \n",
    "### Exploration: Which Airports have the Most Delays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "groupedDelay = sqlContext.sql(\"SELECT Origin, count(*) conFlight,avg(DepDelay) delay \\\n",
    "                                FROM airlineDF \\\n",
    "                                GROUP BY Origin\")\n",
    "\n",
    "df_origin = groupedDelay.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notice:__ To map each Airport to corresponding _Long_ and _Lat_, run the following cell to download the needed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Will download airports.dat if not found in /resources/\n",
    "\n",
    "if os.path.isfile(\"/resources/airports1.dat\") != True:\n",
    "    #If file does not already exist, download it\n",
    "    !wget  --quiet --output-document /resources/airports.dat \\\n",
    "        https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n",
    "    print \"Downloaded to /resources/airports.dat\"\n",
    "else:\n",
    "    #If file already exists\n",
    "    print \"airports.dat already exists under /resources/airports.dat\"\n",
    "    print \"You can continue to the next cell.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/resources/airports.dat', index_col=0,\\\n",
    "names = ['name', 'city', 'country','IATA','ICAO','lat','lng','alt','TZone','DST','Tz'], \\\n",
    "            header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_airports = pd.merge(df_origin, df, left_on = 'Origin', right_on = 'IATA')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_airports.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def zscore(x):\n",
    "    return (x-np.average(x))/np.std(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams['figure.figsize'] = (14,10)\n",
    "\n",
    "\n",
    "my_map = Basemap(projection='merc',\n",
    "            resolution = 'l', area_thresh = 1000.0,\n",
    "            llcrnrlon=-130, llcrnrlat=22, #min longitude (llcrnrlon) and latitude (llcrnrlat)\n",
    "            urcrnrlon=-60, urcrnrlat=50) #max longitude (urcrnrlon) and latitude (urcrnrlat)\n",
    "\n",
    "my_map.drawcoastlines()\n",
    "my_map.drawcountries()\n",
    "my_map.drawmapboundary()\n",
    "my_map.fillcontinents(color = 'white', alpha = 0.3)\n",
    "my_map.shadedrelief()\n",
    "\n",
    "# To create a color map\n",
    "colors = plt.get_cmap('hot')(np.linspace(0.0, 1.0, 30))\n",
    "colors=np.flipud(colors)\n",
    "\n",
    "#----- Scatter -------\n",
    "countrange=max(df_airports['conFlight'])-min(df_airports['conFlight'])\n",
    "al=np.array([sigmoid(x) for x in zscore(df_airports['delay'])])\n",
    "xs,ys = my_map(np.asarray(df_airports['lng']), np.asarray(df_airports['lat']))\n",
    "val=df_airports['conFlight']*4000.0/countrange\n",
    "\n",
    "my_map.scatter(xs, ys,  marker='o', s= val, alpha = 0.8,color=colors[(al*20).astype(int)])\n",
    "\n",
    "#----- Text -------\n",
    "df_text=df_airports[(df_airports['conFlight']>60000) & (df_airports['IATA'] != 'HNL')]\n",
    "xt,yt = my_map(np.asarray(df_text['lng']), np.asarray(df_text['lat']))\n",
    "txt=np.asarray(df_text['IATA'])\n",
    "zp=zip(xt,yt,txt)\n",
    "for row in zp:\n",
    "    #print zp[2]\n",
    "    plt.text(row[0],row[1],row[2], fontsize=10, color='blue',)\n",
    "\n",
    "print(\"Each marker is an airport.\")\n",
    "print(\"Size of markers: Airport Traffic (larger means higher number of flights in year)\")\n",
    "print(\"Color of markers: Average Flight Delay (Redder means longer delays)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exploration: Route delay\n",
    "\n",
    "#### Which Routes are typically the most delayed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp_rout_Delay = sqlContext.sql(\"SELECT Origin, Dest, count(*) traffic,avg(Distance) avgDist,\\\n",
    "                                    avg(DepDelay) avgDelay\\\n",
    "                                FROM airlineDF \\\n",
    "                                GROUP BY Origin,Dest\")\n",
    "rout_Delay = grp_rout_Delay.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_airport_rout1 = pd.merge(rout_Delay, df, left_on = 'Origin', right_on = 'IATA')\n",
    "df_airport_rout2 = pd.merge(df_airport_rout1, df, left_on = 'Dest', right_on = 'IATA')\n",
    "df_airport_rout = df_airport_rout2[[\"Origin\",\"lat_x\",\"lng_x\",\"Dest\",\"lat_y\",\"lng_y\",\\\n",
    "                                    \"avgDelay\", \"traffic\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (14,10)\n",
    "\n",
    "\n",
    "my_map = Basemap(projection='merc',\n",
    "            resolution = 'l', area_thresh = 1000.0,\n",
    "            llcrnrlon=-130, llcrnrlat=22, #min longitude (llcrnrlon) and latitude (llcrnrlat)\n",
    "            urcrnrlon=-60, urcrnrlat=50) #max longitude (urcrnrlon) and latitude (urcrnrlat)\n",
    "\n",
    "my_map.drawcoastlines()\n",
    "my_map.drawcountries()\n",
    "my_map.drawmapboundary()\n",
    "my_map.fillcontinents(color = 'white', alpha = 0.3)\n",
    "my_map.shadedrelief()\n",
    "\n",
    "delay=np.array([sigmoid(x) for x in zscore(df_airports[\"delay\"])])\n",
    "colors = plt.get_cmap('hot')(np.linspace(0.0, 1.0, 40))\n",
    "colors=np.flipud(colors)\n",
    "xs,ys = my_map(np.asarray(df_airports['lng']), np.asarray(df_airports['lat']))\n",
    "xo,yo = my_map(np.asarray(df_airport_rout['lng_x']), np.asarray(df_airport_rout['lat_x']))\n",
    "xd,yd = my_map(np.asarray(df_airport_rout['lng_y']), np.asarray(df_airport_rout['lat_y']))\n",
    "\n",
    "my_map.scatter(xs, ys,  marker='o',  alpha = 0.8,color=colors[(delay*20).astype(int)])\n",
    "\n",
    "\n",
    "al=np.array([sigmoid(x) for x in zscore(df_airport_rout[\"avgDelay\"])])\n",
    "f=zip(xo,yo,xd,yd,df_airport_rout['avgDelay'],al)\n",
    "for row in f:\n",
    "    plt.plot([row[0],row[2]], [row[1],row[3]],'-',alpha=0.07, \\\n",
    "             color=colors[(row[5]*30).astype(int)] )\n",
    "    \n",
    "\n",
    "for row in zp:\n",
    "    plt.text(row[0],row[1],row[2], fontsize=10, color='blue',)\n",
    "\n",
    "print(\"Each line represents a route from the Origin to Destination airport.\")\n",
    "print(\"The redder line, the higher probablity of delay.\")\n",
    "    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration: Airport Origin delay per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the airport code name below to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Origin_Airport=\"JFK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ORG = sqlContext.sql(\"SELECT * from airlineDF WHERE origin='\"+ Origin_Airport+\"'\")\n",
    "df_ORG.registerTempTable(\"df_ORG\")\n",
    "df_ORG.select('ArrTime','CRSArrTime','CRSDepTime',\\\n",
    "              'DayOfWeek','DayofMonth','DepDelay','DepTime','Dest').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at flights originating from this airport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"total flights from this ariport: \" + str(df_ORG.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we group flights by month to see how delayed flights are distributed by month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grp_carr = sqlContext.sql(\"SELECT  UniqueCarrier,month, avg(DepDelay) avgDelay from df_ORG \\\n",
    "                            WHERE DepDelayed=True \\\n",
    "                            GROUP BY UniqueCarrier,month\")\n",
    "s = grp_carr.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps = s.pivot(index='month', columns='UniqueCarrier', values='avgDelay')[['AA','UA','US']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (8,5)\n",
    "ps.plot(kind='bar', colormap='Greens');\n",
    "plt.xlabel('Average delay')\n",
    "plt.ylabel('Month')\n",
    "plt.title('How much delay does each carrier has in each month?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that average delay in this year is is highest in June and August in this airport."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration: Airport Origin delay per day/hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hour_grouped = df_ORG.filter(df_ORG['DepDelayed']).select('DayOfWeek','hour','DepDelay').groupby('DayOfWeek','hour').mean('DepDelay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (10,5)\n",
    "dh = hour_grouped.toPandas()\n",
    "c = dh.pivot('DayOfWeek','hour')\n",
    "X = c.columns.levels[1].values\n",
    "Y = c.index.values\n",
    "Z = c.values\n",
    "plt.xticks(range(0,24), X)\n",
    "plt.yticks(range(0,7), Y)\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Day of Week')\n",
    "plt.title('Average delay per hours and day?')\n",
    "plt.imshow(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clear pattern here: flights tend to be delayed in these situations:  \n",
    "- Later in the day: possibly because delays tend to pile up as the day progresses and the problem tends to compound later in the day.  \n",
    "- Mornings in first day of week possibly because of more business meetings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Logistic Regression\n",
    "In this section, we will build a supervised learning model to predict flight delays for flights leaving our selected airport.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "In the next two cell we select the features that we need to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model=df_ORG\n",
    "stringIndexer2 = StringIndexer(inputCol=\"Dest\", outputCol=\"destIndex\")\n",
    "model_stringIndexer = stringIndexer2.fit(df_model)\n",
    "indexedDest = model_stringIndexer.transform(df_model)\n",
    "encoder2 = OneHotEncoder(dropLast=False, inputCol=\"destIndex\", outputCol=\"destVec\")\n",
    "df_model = encoder2.transform(indexedDest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assembler\n",
    "In order to train our logistic regression model, we have to combine features generated above into a single feature vector. _VectorAssembler_ is a transformer that combines a given list of columns into a single vector column. In each row, the values of the input columns will be concatenated into a vector in the specified order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols = ['Year','Month','DayofMonth','DayOfWeek','hour','Distance','destVec'],\n",
    "    outputCol = \"features\")\n",
    "df_assembled = assembler.transform(df_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "In the following cell, we use _MinMaxScaler_ to rescale each feature to a specific range  [0, 1] which is appropriate for Logistic Regression in TensorFlow. MinMaxScaler computes summary statistics on a data set and produces a _MinMaxScalerModel_. The model can then transform each feature individually such that it is in the given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minmaxscaler= MinMaxScaler(inputCol=\"features\", outputCol=\"minMaxFeatures\")\n",
    "minMaxModel = minmaxscaler.fit(df_assembled)\n",
    "minMax_df = minMaxModel.transform(df_assembled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling\n",
    "The corresponding labels in Flight dataset are True and False describing flight delay which is either more or less than 15 seconds. For the purposes of this tutorial, we are going to want our labels as __one-hot vectors__. A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the no-delay (False) will be represented as a vector which is one in the first dimension, i.e. [1.0,0.0], and the True label will be represented as [0.0,1.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "def delay_lbl(x):\n",
    "    return [[1.0,0.0],[0.0,1.0]][x]\n",
    "\n",
    "func = udf(delay_lbl, ArrayType(FloatType(),True))\n",
    "labeled_df = minMax_df.withColumn('label', func(minMax_df.DepDelayed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Numpy matrix\n",
    "We will convert each row of the dataframe into a vector of 1x76 array. The result is that our cleaned dataset is a tensor (an n-dimensional array) with a shape of [7M, 76]. The first dimension indexes the flights and the second dimension indexes the features including Year, Month, Day, Destinatin, etc. Each entry in the tensor has a value between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1=labeled_df.select('label','minMaxFeatures').toPandas()\n",
    "np_data_mtx=np.matrix(d1['minMaxFeatures'].map(lambda r:r.toArray().tolist()).tolist())\n",
    "np_lbl_mtx=np.matrix(d1['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting dataset into train and test dtasets\n",
    "The data is split into two parts, 70% of data as training data, and 30% as test data. This split is very important: it's essential in machine learning that we have separate data which we don't learn from so that we can make sure that what we've learned actually generalizes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rowCount,colCount=np_data_mtx.shape\n",
    "testPercent=int(rowCount*0.3)\n",
    "ix=np.random.randint(0,rowCount,testPercent)\n",
    "mask = np.ones(rowCount,dtype=bool) \n",
    "mask[ix] = False\n",
    "tr,ts=np_data_mtx[mask],np_data_mtx[~mask]\n",
    "tr_lbl,ts_lbl=np_lbl_mtx[mask],np_lbl_mtx[~mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "We want out model be able to look at a flight and give probabilities for it haing delay. For example, our model might look at a flight scheduled at 8:00 PM of Monday, March 2nd and be 60% sure it is going to be on-time, but give a 40% chance to it be delayed. This is a classic case where a __logistic regression__ is used as a classification model. In logistic regression, first we add up the evidence of our input being in certain classes, and then we convert that evidence into probabilities.\n",
    "\n",
    "In the following cell, we set parameters after importing TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Set the Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "classCount=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We create two placeholders _x_ and _y_ which are values that we will input when we ask TensorFlow to run a computation. We want to be able to input any number of flights, represented by a colCount-dimensional vector. We represent this as a 2-D tensor of floating-point numbers, with a shape [None, colCount]. (Here None means that a dimension can be of any length.) __x__ represents the feature list and _'y_' is a placeholder to input the correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, colCount])\n",
    "y_ = tf.placeholder(tf.float32, [None, classCount])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to define weights and biases variables for our model. A __Variable__ in TensorFlow is a modifiable tensor that lives in TensorFlow's graph of interacting operations. It can be used and even modified by the computation.  \n",
    "To make the Logistic Regression we use _Softmax_ function which is a generalized version of Logistic Regressiona and can be used for multi class classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([colCount, classCount]))\n",
    "b = tf.Variable(tf.zeros([classCount]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our model, we need to define the cost function, and then try to minimize it. In this cell, we use \"cross-entropy\" which is widely used in machine learning.  TensorFlow trains the model using a backpropagation algorithm to efficiently determine how our variables affect the cost that should be minimized. In this case, TensorFlow uses the _gradient descent algorithm_ to minimize _cross entropy_. Gradient descent is a simple procedure, where TensorFlow simply shifts each variable a little bit in the direction that reduces the cost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just have set up our model so far to train. Now, we have to add an operation to initialize the variables we created, and then we launch the model in a Session, and run the operation that initializes the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we will run the training step for 25 cycles. In each cycle, we selct batches many times and train the model and at the end compute the average loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(rowCount/batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        r=np.random.randint(0,tr.shape[0],batch_size)\n",
    "        batch_xs=tr[r]\n",
    "        batch_ys = tr_lbl[r]\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cross_entropy, feed_dict={x: batch_xs,y_: batch_ys})/total_batch\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Let's figure out where we predicted the correct label. __tf.argmax__ gives you the index of the highest entry in a tensor along some axis. For example, tf.argmax(y,1) is the label our model thinks is most likely for each input, while tf.argmax(y_,1) is the correct label. We can use tf.equal to check if our prediction matches the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "acc=((sess.run(accuracy, feed_dict={x: ts, y_: ts_lbl}))*100)\n",
    "print \"Model Accuracy for JFK: %1.2f %%\" % acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model to predict your flight from JFK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following widget to query the model.  \n",
    "For example the following flight has dely:  \n",
    "    Month=2, Day=3, Hour=18, Dest=CLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Destin = rout_Delay[rout_Delay['Origin']=='JFK'].Dest.unique()\n",
    "pred=tf.argmax(y,1)\n",
    "@interact(Destination=tuple(Destin),Month=(1,12),DayofMonth=(1,30),DayOfWeek=(0,7),Hour=(0,23))\n",
    "def g(Destination,Month,DayofMonth,DayOfWeek,Hour):\n",
    "    Distance=int(rout_Delay[(rout_Delay['Origin']=='JFK') & (rout_Delay['Dest']==Destination)]\\\n",
    "                 .avgDist.tolist()[0])\n",
    "    testcase=  Row(Year=2007.0,Month=Month,DayofMonth=DayofMonth,DayOfWeek=DayOfWeek,hour=Hour,\\\n",
    "                 Origin='JFK',Dest=Destination,Distance=Distance) \n",
    "    TestCase_df = sqlContext.createDataFrame(sc.parallelize([testcase]))\n",
    "    t1 = model_stringIndexer.transform(TestCase_df)\n",
    "    t2 = encoder2.transform(t1)\n",
    "    t3 = assembler.transform(t2)\n",
    "    t4 = minMaxModel.transform(t3)\n",
    "    case=t4.select('minMaxFeatures').take(1)[0]['minMaxFeatures']\n",
    "    case2=np.asmatrix(case)\n",
    "    p=sess.run(pred, feed_dict={x: case2})\n",
    "    print \"Flight from JFK to \"+Destination + \", Distance:\" + str(Distance)\n",
    "    if p==0:\n",
    "        print \"You flight doesnt have a delay, Accuracy= %1.2f %%\" % (acc)\n",
    "    else:\n",
    "        print \"You flight may be delayed, Accuracy= %1.2f %%\" % (acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://bigdatauniversity.com/courses/advanced-classification-and-prediction/?utm_source=tutorial-flightdelay-tensor&utm_medium=dswb&utm_campaign=bdu\"><img src = \"https://ibm.box.com/shared/static/u7iyiej98gb971gmjqvfsveqz3ik4fxj.png\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Authors:</h3>\n",
    "<article class=\"teacher\">\n",
    "<div class=\"teacher-image\" style=\"    float: left;\n",
    "    width: 115px;\n",
    "    height: 115px;\n",
    "    margin-right: 10px;\n",
    "    margin-bottom: 10px;\n",
    "    border: 1px solid #CCC;\n",
    "    padding: 3px;\n",
    "    border-radius: 3px;\n",
    "    text-align: center;\"><img class=\"alignnone wp-image-2258 \" src=\"https://ibm.box.com/shared/static/tyd41rlrnmfrrk78jx521eb73fljwvv0.jpg\" alt=\"Saeed Aghabozorgi\" width=\"178\" height=\"178\" /></div>\n",
    "<h4>Saeed Aghabozorgi</h4>\n",
    "<p><a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, PhD is a Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clients’ ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>\n",
    "</article>\n",
    "<article class=\"teacher\">\n",
    "<div class=\"teacher-image\" style=\"    float: left;\n",
    "    width: 115px;\n",
    "    height: 115px;\n",
    "    margin-right: 10px;\n",
    "    margin-bottom: 10px;\n",
    "    border: 1px solid #CCC;\n",
    "    padding: 3px;\n",
    "    border-radius: 3px;\n",
    "    text-align: center;\"><img class=\"alignnone size-medium wp-image-2177\" src=\"https://ibm.box.com/shared/static/2ygdi03ahcr97df2ofrr6cf8knq4kodd.jpg\" alt=\"Polong Lin\" width=\"300\" height=\"300\" /></div>\n",
    "<h4>Polong Lin</h4>\n",
    "<p>\n",
    "<a href=\"https://ca.linkedin.com/in/polonglin\">Polong Lin</a> is a Data Scientist at IBM in Canada. Under the Emerging Technologies division, Polong is responsible for educating the next generation of data scientists through Big Data University. Polong is a regular speaker in conferences and meetups, and holds a M.Sc. in Cognitive Psychology.</p>\n",
    "</article>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<hr>\n",
    "Copyright &copy; 2016 [Big Data University](https://bigdatauniversity.com/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
