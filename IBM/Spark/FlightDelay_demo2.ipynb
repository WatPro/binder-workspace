{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo2: Flight delay prediction - Submitting to the Spark Sandbox cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Warning__: Do not run this notebook locally! Read the instructions below for how to submit this notebook to a remote Spark cluster. Running this notebook locally will take a long time and run out of available memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous tutorial, we used the __Flight Dataset__ to analyze and predict flight delays based on past flights. We showed how you can use Jupyter Notebook and Spark to read, explore, analyze and visualize your resutls. The dataset that we used in the previous tutorial was the flights related to 2007, containing 7 Million flights.\n",
    "\n",
    "Can we improve the results?\n",
    "\n",
    "Yes, the prediction accuracy can be improved by using a much bigger data set. In this tutorial you will learn how to __submit a Jupyter notebook__ for execution on a remote spark cluster. Because of the large data set, his is a batch execution rather than an interactive demo.\n",
    "\n",
    "- We will use the [Flight Dataset](http://stat-computing.org/dataexpo/2009/the-data.html) that is already acceccible in a shared space. We will build a classification model to predicts airline delay from historial flight data. This dataset is 5 GB and contains 52 million flight records. Processing of such a large data set requires use of a Spark cluster. \n",
    "\n",
    "- This Notebook will operate on the data set stored in the HDFS file system of our Spark Sandbox cluster.\n",
    "\n",
    "First, we import some python packages that we need for this use-case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.param import Param, Params\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Getting the data and creating the RDD\n",
    "As mentioned, we will use the complete dataset, containing nearly 50 million flights. The file is provided as a __.csv file__ that we have access through DSWB. Size of this dataset is 5 GB. We read data from __HDFS__ into an __RDD__.\n",
    "\n",
    "This data is already preloaded into HDFS on the Spark Sandbox cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "textFile = sc.textFile(\"hdfs://spark.datascientistworkbench.com:9000/sample_data/2001-2008-merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Caching\n",
    "In this section, we remove the header of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFileRDD=textFile.map(lambda x: x.split(','))\n",
    "header = textFileRDD.first()\n",
    "textRDD = textFileRDD.filter(lambda r: r != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataframe from RDD\n",
    "A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in Python, but with richer optimizations under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(r):\n",
    "    try:\n",
    "        x=Row(Year=int(r[0]),\\\n",
    "          Month=int(r[1]),\\\n",
    "          DayofMonth=int(r[2]),\\\n",
    "          DayOfWeek=int(r[3]),\\\n",
    "          DepTime=int(float(r[4])), \\\n",
    "          CRSDepTime=int(r[5]),\\\n",
    "          ArrTime=int(float(r[6])),\\\n",
    "          CRSArrTime=int(r[7]), \\\n",
    "          UniqueCarrier=r[8],\\\n",
    "          DepDelay=int(float(r[15])),\\\n",
    "          Origin=r[16],\\\n",
    "          Dest=r[17], \\\n",
    "          Distance=int(float(r[18])))  \n",
    "    except:\n",
    "        x=None  \n",
    "    return x\n",
    "rowRDD=textRDD.map(lambda r: parse(r)).filter(lambda r:r != None)\n",
    "airline_df = sqlContext.createDataFrame(rowRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we add a new column to our data frame to determine the delayed flight against non-delayed ones. Later, we use this column as target/label column in the classification process. So, a binary variable is defined as __DepDelayed__ which its value __True__ for flights having 15 mins or more of delay, and __False__ otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "airline_df=airline_df.withColumn('DepDelayed',airline_df['DepDelay']>15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define hour function to obtain hour of day\n",
    "def hour_ex(x): \n",
    "    h=int(str(int(x)).zfill(4)[:2])\n",
    "    return h\n",
    "# register as a UDF \n",
    "f = udf(hour_ex, IntegerType())\n",
    "#CRSDepTime: scheduled departure time (local, hhmm)\n",
    "airline_df=airline_df.withColumn('hour', f(airline_df.CRSDepTime))\n",
    "airline_df.registerTempTable(\"airlineDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Airport selection\n",
    "In the following cell we select the airline that we want to predict its delay. To simplify, we will build a supervised learning model to predict flight delays for flights leaving JFK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Origin_Airport=\"JFK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_ORG =sqlContext.sql(\"SELECT * from airlineDF WHERE Origin='\"+ Origin_Airport+\"'\")\n",
    "df_ORG.registerTempTable(\"df_ORG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Feature selection\n",
    "In the next two cell we select the featurs that we need to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_model=df_ORG\n",
    "stringIndexer1 = StringIndexer(inputCol=\"Origin\", outputCol=\"originIndex\")\n",
    "model_stringIndexer = stringIndexer1.fit(df_model)\n",
    "indexedOrigin = model_stringIndexer.transform(df_model)\n",
    "encoder1 = OneHotEncoder(dropLast=False, inputCol=\"originIndex\", outputCol=\"originVec\")\n",
    "df_model = encoder1.transform(indexedOrigin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use __labeled point__ to make local vectors associated with a label/response. In MLlib, labeled points are used in supervised learning algorithms and they are stored as doubles. For binary classification, a label should be either 0 (negative) or 1 (positive). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['Year','Month','DayofMonth','DayOfWeek','hour','Distance','originVec'],\n",
    "    outputCol=\"features\")\n",
    "output = assembler.transform(df_model)\n",
    "airlineRDD=output.map(lambda row: LabeledPoint([0,1][row['DepDelayed']],row['features']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: Spliting dataset into train and test dtasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainRDD,testRDD=airlineRDD.randomSplit([0.7,0.3])\n",
    "model = LogisticRegressionWithLBFGS.train(trainRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluating the model on testing data\n",
    "labelsAndPreds = testRDD.map(lambda p: (p.label, model.predict(p.features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conf(r):\n",
    "    if r[0] == r[1] ==1: x= 'TP'\n",
    "    if r[0] == r[1] ==0: x= 'TN'\n",
    "    if r[0] == 1 and  r[1] ==0: x= 'FN'\n",
    "    if r[0] == 0 and  r[1] ==1: x= 'FP'\n",
    "    return (x)\n",
    "acc1=labelsAndPreds.map(lambda (v, p): ((v, p),1)).reduceByKey(lambda a, b: a + b).take(5)\n",
    "acc=[(conf(x[0]),x[1]) for x in acc1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TP=TN=FP=FN=0.0\n",
    "for x in acc: \n",
    "    if x[0]=='TP': TP= x[1]\n",
    "    if x[0]=='TN': TN= x[1]\n",
    "    if x[0]=='FP': FP= x[1]\n",
    "    if x[0]=='FN': FN= x[1]\n",
    "eps=sys.float_info.epsilon\n",
    "Accuracy= (TP+TN) / (TP + TN+ FP+FN+eps) \n",
    "print \"Model Accuracy for JFK: %1.2f %%\" % (Accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to Spark Cluster\n",
    "\n",
    "When you create a notebook in the Data Scientist Workbench, you have full access to a local Spark. Local Spark is very useful for building code and working with small datasets.\n",
    "\n",
    "When working with large datasets, you will need the power of a Spark cluster to finish your jobs in a reasonable time.\n",
    "\n",
    "To submit your Python notebook to a *Spark cluster* for execution, expand the twistie next to the notebook name in the sidebar and click on *Submit to Spark Cluster* menu item. Please see the screenshot below.\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/2hh6wr5o03sldyt3k33h2zxf1a3ekxnb.png\"/>\n",
    "\n",
    "### Warning\n",
    "  \"Submit notebook to Spark Cluster\" will open a new notebook named \"Submit-to-Spark-ClusterX\". Follow the instructions in the new notebook to run this notebook on the remote Spark cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Want to learn more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://bigdatauniversity.com/courses/spark-fundamentals/?utm_source=tutorial-flight-demo-2&utm_medium=dswb&utm_campaign=bdu\"><img src = \"https://ibm.box.com/shared/static/r3pj5oo2ivnzqar0poj2eexiqrnvq6vy.png\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://bigdatauniversity.com/courses/advanced-classification-and-prediction/?utm_source=tutorial-flight-demo-2&utm_medium=dswb&utm_campaign=bdu\"><img src = \"https://ibm.box.com/shared/static/u7iyiej98gb971gmjqvfsveqz3ik4fxj.png\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "<h3>Authors:</h3>\n",
    "<article class=\"teacher\">\n",
    "<div class=\"teacher-image\" style=\"    float: left;\n",
    "    width: 115px;\n",
    "    height: 115px;\n",
    "    margin-right: 10px;\n",
    "    margin-bottom: 10px;\n",
    "    border: 1px solid #CCC;\n",
    "    padding: 3px;\n",
    "    border-radius: 3px;\n",
    "    text-align: center;\"><img class=\"alignnone wp-image-2258 \" src=\"https://ibm.box.com/shared/static/tyd41rlrnmfrrk78jx521eb73fljwvv0.jpg\" alt=\"Saeed Aghabozorgi\" width=\"178\" height=\"178\" /></div>\n",
    "<h4>Saeed Aghabozorgi</h4>\n",
    "<p><a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, PhD is a Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clientsâ€™ ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>\n",
    "</article>\n",
    "<article class=\"teacher\">\n",
    "<div class=\"teacher-image\" style=\"    float: left;\n",
    "    width: 115px;\n",
    "    height: 115px;\n",
    "    margin-right: 10px;\n",
    "    margin-bottom: 10px;\n",
    "    border: 1px solid #CCC;\n",
    "    padding: 3px;\n",
    "    border-radius: 3px;\n",
    "    text-align: center;\"><img class=\"alignnone size-medium wp-image-2177\" src=\"https://ibm.box.com/shared/static/2ygdi03ahcr97df2ofrr6cf8knq4kodd.jpg\" alt=\"Polong Lin\" width=\"300\" height=\"300\" /></div>\n",
    "<h4>Polong Lin</h4>\n",
    "<p>\n",
    "<a href=\"https://ca.linkedin.com/in/polonglin\">Polong Lin</a> is a Data Scientist at IBM in Canada. Under the Emerging Technologies division, Polong is responsible for educating the next generation of data scientists through Big Data University. Polong is a regular speaker in conferences and meetups, and holds a M.Sc. in Cognitive Psychology.</p>\n",
    "</article>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created by: <a href=\"https://bigdatauniversity.com/?utm_source=bducreatedbylink&utm_medium=dswb&utm_campaign=bdu\">The Cognitive Class Team</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}